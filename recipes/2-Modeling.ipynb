{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-us_census_project-vcpu-2-ram-16gib",
      "display_name": "Python in vCPU-2-RAM-16GiB (env us_census_project)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "daniel.t.soukup@gmail.com",
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "daniel.t.soukup@gmail.com"
      },
      "lastModifiedOn": 1762039630760
    },
    "createdOn": 1762039630760,
    "creator": "daniel.t.soukup@gmail.com",
    "tags": [
      "recipe-editor"
    ],
    "customFields": {},
    "associatedRecipe": "2-Modeling-recipe"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling and Fine Tuning\n\n**Owner: Daniel Soukup - Created: 2025.11.01**\n\nIn this notebook, we load the processed data and fit our models focusing on optimizing variations of XGBoost classifiers across multiple hyperparameters that balance variance and bias, while addressing the class imbalance discussed during EDA. We chose to go in-depth on comparing variations of this single model type to allow focus on the details.\n\n**NOTE:** due to randomness in the model fitting and tuning process, rerunning the notebook might change the outputs (such as top predictors) and add inconsistencies with the current markdown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s load our processed data and create feature/target dataframes for both train and test."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\n# Read recipe inputs\nprocessed_learn \u003d dataiku.Dataset(\"processed_learn\")\nprocessed_learn_df \u003d processed_learn.get_dataframe()\n\nprocessed_test \u003d dataiku.Dataset(\"processed_test\")\nprocessed_test_df \u003d processed_test.get_dataframe()\n\nprocessed_learn_df.shape, processed_test_df.shape"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "processed_learn_df.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We notice that some special characters can cause issues with our model - we address this here."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "processed_learn_df.columns \u003d [col.replace(\"\u003c\", \"less\").replace(\"\u003e\", \"more\") for col in processed_learn_df.columns]\nprocessed_test_df.columns \u003d [col.replace(\"\u003c\", \"less\").replace(\"\u003e\", \"more\") for col in processed_test_df.columns]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TARGET \u003d \u0027income\u0027"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train, y_train \u003d processed_learn_df.drop(columns\u003dTARGET), processed_learn_df[TARGET]\nX_test, y_test \u003d processed_test_df.drop(columns\u003dTARGET), processed_test_df[TARGET]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall that 8% of the processed samples fall into the target class 1 (high income) so a dummy classifier predicting 0 only would be 92% accurate."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_train.mean()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Important Note:** We won\u0027t use the test set for any optimization to avoid overfitting, we reserve the test set for final evaluation only of the optimized model as an unbiased estimate of our model on completely unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modeling\n\nOur current approach will focus on optimizing XGBoost binary classifiers. We do this using Optuna to search the hyperparameter space efficiently. We also aim to address the class imbalance during the training by:\n- using stratified splitting for cross-validation,\n- adjusting the evaluation metric from accuracy to AUC-PR (to select the best HP for a balance of high precision and recall),\n- experimenting with class-balancing methods, such as class weights tuned with other hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment Tracking\n\nWe will be recording our models and detailed metrics under two main experiments that will capture multiple runs."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "project \u003d dataiku.api_client().get_default_project()\nmanaged_folder \u003d project.get_managed_folder(\u0027lV6oqreY\u0027)\n\nTUNING_XP \u003d \"xgboost_hp_tuning\"\nBASELINE_XP \u003d \"baseline_xp\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned, we\u0027ll be using sample weights to adjust for the class imbalance:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.utils.class_weight import compute_sample_weight\nfrom typing import Union\n\ndef get_sample_weights(multiplier: Union[int, None]) -\u003e np.array:\n    \"\"\"\n    Weight the minority sample higher to contribute more to the training loss.\n    \"\"\"\n    if multiplier:\n        return compute_sample_weight({0: 1, 1: multiplier}, y_train)\n    else:\n        return None\n\nweights \u003d get_sample_weights(10)\nweights"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to compare model variations, we need to split the train set into train and validation. For this, we set up our cross-validation helper and define base parameters for our model:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from typing import Dict, Any\nimport xgboost as xgb\n\ndef cross_val_score_xgb(param: Dict[str, Any]) -\u003e float:\n    \"\"\"\n    Fit model with 3-fold cross validation using the provided params.\n\n    Return the avg out-of-fold metric as specified in the provided params.\n    \"\"\"\n    dtrain \u003d xgb.DMatrix(\n        X_train,\n        label\u003dy_train,\n        weight\u003dget_sample_weights(param.get(\u0027multiplier\u0027)),\n    )\n\n    results \u003d xgb.cv(\n        params\u003dparam,\n        dtrain\u003ddtrain,\n        num_boost_round\u003dparam.get(\"n_estimators\"), # default 10\n        nfold\u003d3,\n        seed\u003d42,\n        verbose_eval\u003dFalse,\n        stratified\u003dparam.get(\"stratified_cv\"), # default False\n    )\n\n    return results\n\n# we wont change these\nBASE_PARAMS \u003d {\n    \"verbosity\": 0,\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": \"aucpr\", # adjusted for the imbalance\n    \"stratified_cv\": True # adjusted for the imbalance\n}\n\nparam \u003d BASE_PARAMS.copy()\nparam.update({\n    \"n_estimators\": 10,\n    \"max_depth\": 2,\n    \"multiplier\": 1,\n    }\n)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets test our function with logging the run:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "def run_cv_with_logging(param: dict) -\u003e pd.DataFrame:\n    \"\"\"\n    Log the CV run with MLflow to BASELINE_XP:base_run.\n    \n    Model are not saved unsave autologged.\n    \"\"\"\n    \n    with project.setup_mlflow(managed_folder\u003dmanaged_folder) as mlflow_handle:\n        mlflow_handle.set_experiment(BASELINE_XP)\n\n        with mlflow_handle.start_run(run_name\u003d\"base_run\", nested\u003dTrue):\n            result \u003d cross_val_score_xgb(param)\n\n            best_score \u003d result[\"test-aucpr-mean\"].values[-1]\n\n            # logging\n            mlflow_handle.log_params(param)\n            mlflow_handle.log_metrics(\n                {\n                    \u0027best_score\u0027: best_score\n                }\n            )\n            \n    return result\n\nresults \u003d run_cv_with_logging(param)\nresults.tail(3)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets try with a large multiplier:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "param \u003d BASE_PARAMS.copy()\nparam.update({\n    \"n_estimators\": 10,\n    \"max_depth\": 2,\n    \"multiplier\": 10,\n    }\n)\n\nresults \u003d run_cv_with_logging(param)\nresults.tail(3)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the multiplier has a massive effect on the aucpr score. \n\nWe can also test the recommended `scale_pos_weight` parameter that helps balance classes. A typical value to consider based on the XGBoost recommendations is `sum(negative instances) / sum(positive instances)` and is supposed to assign a weight independent of the sample for the whole positive class - we\u0027re supposed to get similar results."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scale_pos_weight \u003d (1 - y_train).sum()/y_train.sum()\nscale_pos_weight"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "param \u003d BASE_PARAMS.copy()\nparam.update({\n    \"n_estimators\": 10,\n    \"max_depth\": 2,\n    \"scale_pos_weight\": scale_pos_weight\n    }\n)\n\nresults \u003d run_cv_with_logging(param)\nresults.tail(3)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interestingly, we don\u0027t see as much of a difference so we\u0027ll leave this and explore in the future."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "BASE_PARAMS.update({\"scale_pos_weight\": scale_pos_weight})"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimize Hyperparameters - Main Run\n\nNext, we\u0027ll look to optimize the model hyperparameters more systematically. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function below defines the HP space to explore (parameters and their ranges), focusing on 5 such parameters with known strong effect on model performance and regularization:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def objective(trial) -\u003e float:\n    \"\"\"\n    Capture a single param combination and model fitting,\n    evaluated using cross-validation.\n    \"\"\"\n    param \u003d BASE_PARAMS.copy()\n    param.update({\n        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0), # default 1 - all rows\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0), # default 1 - all columns\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 100, step\u003d10), # default 100\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 20, step\u003d2), # default 3\n        \"multiplier\": trial.suggest_int(\"multiplier\", 1, 50)\n    })\n    \n    with mlflow_handle.start_run(run_name\u003d\"trial\", nested\u003dTrue):\n        result \u003d cross_val_score_xgb(param)\n        best_score \u003d result[\"test-aucpr-mean\"].values[-1]\n        \n        # logging\n        mlflow_handle.log_params(param)\n        mlflow_handle.log_metrics(\n            {\n                \u0027best_score\u0027: best_score\n            }\n        )\n        \n        return best_score"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we are ready to run our study, currently consisting of 40 trials:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "from xgboost import XGBClassifier\nimport optuna\n\nN_TRIALS \u003d 40\n\nwith project.setup_mlflow(managed_folder\u003dmanaged_folder) as mlflow_handle:\n    \n    mlflow_handle.set_experiment(TUNING_XP)\n     \n    with mlflow_handle.start_run(run_name\u003d\"study\", nested\u003dTrue) as study_run:\n        study \u003d optuna.create_study(direction\u003d\"maximize\")\n        study.optimize(objective, n_trials\u003dN_TRIALS, timeout\u003d600)\n        \n        # logging\n        best_params \u003d study.best_trial.params\n        mlflow_handle.log_metrics(\n            {\n                \u0027best_score\u0027: study.best_trial.value\n            }\n        )\n        \n        # refit best model\n        model \u003d XGBClassifier(**study.best_trial.params)\n        model \u003d model.fit(X_train, y_train)\n        \n        # log best params \u0026 model\n        mlflow_handle.log_params(model.get_xgb_params())\n        mlflow_handle.xgboost.log_model(\n            model,\n            \"xgboost_model\",\n            input_example\u003dX_train.head(10),\n            pip_requirements\u003d[\u0027xgboost\u003d\u003d2.1.1\u0027]\n        )"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets see the best results:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Best trial:\")\ntrial \u003d study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tuning Analysis\n\nLet\u0027s see how the HP choices impacted performance:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "study_df \u003d study.trials_dataframe()\nstudy_df.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will look at different projections of the HP space and the best observed values:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.express as px\nimport plotly.offline as pyo\npyo.init_notebook_mode()\n\npivot \u003d pd.pivot_table(study_df, index\u003d\"params_max_depth\", columns\u003d\"params_n_estimators\", values\u003d\"value\", aggfunc\u003d\u0027max\u0027)\nfig \u003d px.imshow(\n    pivot,\n    color_continuous_scale\u003d\"blues\",\n    title\u003d\"Best values across combinations\"\n)\nfig.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The best performing models were found with the mid-to-higher range of boosting rounds and lower max depth (the latter help avoid overfitting if the number of estimators is high)."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pivot \u003d pd.pivot_table(study_df, index\u003d\"params_max_depth\", columns\u003d\"params_colsample_bytree\", values\u003d\"value\", aggfunc\u003d\u0027max\u0027)\nfig \u003d px.imshow(\n    pivot,\n    color_continuous_scale\u003d\"blues\",\n    title\u003d\"Best values across combinations\",\n\n)\nfig.update_xaxes(\n    scaleanchor\u003d\"x\",\n  )\nfig.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In our experiemnts, the high scores also corresponded with smaller col samples (how many col\u0027s each estimater used) unless max depth was singificantly lowered. The small col sample again helps avoid overfitting although the patterns are maybe less clear."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pivot \u003d pd.pivot_table(study_df, index\u003d\"params_n_estimators\", columns\u003d\"params_colsample_bytree\", values\u003d\"value\", aggfunc\u003d\u0027max\u0027)\nfig \u003d px.imshow(\n    pivot,\n    color_continuous_scale\u003d\"blues\",\n    title\u003d\"Best values across combinations\",\n\n)\nfig.update_xaxes(\n    scaleanchor\u003d\"x\",\n  )\nfig.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While the patterns might not be the most clear here, we can see that having high boosting rounds and high sample leads to lower scores (the bottom right corner, likely overfitting again).\n\nGiven that some of the best results were observed at the end of the specified search range, it would be a good next step to extend the range further, potentially with a larger step size for boosting rounds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we look at the multiplier effect:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.pivot_table(study_df, index\u003d\"params_multiplier\", values\u003d\"value\", aggfunc\u003d\u0027mean\u0027).T"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On average, the higher the multiplier the better the aucpr score we got which is also show in the heatmaps below. It looks like we get the most benefit around \u003e40 weighting."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pivot \u003d pd.pivot_table(study_df, index\u003d\"params_multiplier\", columns\u003d\"params_n_estimators\", values\u003d\"value\", aggfunc\u003d\u0027max\u0027)\nfig \u003d px.imshow(\n    pivot,\n    color_continuous_scale\u003d\"blues\",\n    title\u003d\"Best values across combinations\"\n)\nfig.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This patter is nicely shown the heatmap above and below as well."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pivot \u003d pd.pivot_table(study_df, index\u003d\"params_multiplier\", columns\u003d\"params_max_depth\", values\u003d\"value\", aggfunc\u003d\u0027max\u0027)\nfig \u003d px.imshow(\n    pivot,\n    color_continuous_scale\u003d\"blues\",\n    title\u003d\"Best values across combinations\"\n)\nfig.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predict\n\nWe save the predicted class and probabilities both calculated:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "predictions_learn_df \u003d pd.DataFrame(\n    {\n        TARGET: y_train,\n        \u0027pred\u0027: model.predict(X_train),\n        \u0027pred_proba\u0027: model.predict_proba(X_train)[:, 1]\n    }\n)\n\npredictions_test_df \u003d pd.DataFrame(\n    {\n        TARGET: y_test,\n        \u0027pred\u0027: model.predict(X_test),\n        \u0027pred_proba\u0027: model.predict_proba(X_test)[:, 1]\n    }\n)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally lets look at the feature importances for our model too (top 20):"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig \u003d pd.DataFrame(\n    {\n        \u0027importance\u0027: model.feature_importances_,\n    },\n    index\u003dmodel.feature_names_in_\n).sort_values(\u0027importance\u0027).tail(20).plot(kind\u003d\"bar\", backend\u003d\u0027plotly\u0027)\n\nfig.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observations:\n- the sex dummy variable shows a strong effect on the model (top-5 importance across multiple runs) which aligns with the well-known pay imbalance between genders,\n- employment indicators such as class of worker, cap gains and losses natually showed up as well,\n- education fields (high school and college) showed high in the ranked list,\n- as well as age although not as pronounced.\n\nAll these findings align with our expectations and EDA. Our model picked up on the gender bias in our data (there are much more high earner males in the dataset than female) which can definitely be addressed in future model iterations - please see the slides for more info."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# gender imbalance\nprocessed_learn_df.groupby(TARGET).mean(\"sex_Male\")[\"sex_Male\"]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "79% of high income earners were male, as opposed to 46% of low income. This statistical inparity is a strong signal for the model to pick up on and use for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save predictions\n\nWe finally save the results to their own datasets which can be used for evaluation:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write recipe outputs\npredictions_learn \u003d dataiku.Dataset(\"predictions_learn\")\npredictions_learn.write_with_schema(predictions_learn_df)\n\npredictions_test \u003d dataiku.Dataset(\"predictions_test\")\npredictions_test.write_with_schema(predictions_test_df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "study_data \u003d dataiku.Dataset(\"xgboost_study\")\nstudy_data.write_with_schema(study_df)"
      ],
      "outputs": []
    }
  ]
}