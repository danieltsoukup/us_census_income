{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-us_census_project-vcpu-2-ram-16gib",
      "display_name": "Python in vCPU-2-RAM-16GiB (env us_census_project)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "tags": [
      "recipe-editor"
    ],
    "createdOn": 1762039630760,
    "associatedRecipe": "Modeling",
    "creator": "daniel.t.soukup@gmail.com",
    "customFields": {},
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "daniel.t.soukup@gmail.com"
      },
      "lastModifiedOn": 1762039630760
    },
    "modifiedBy": "daniel.t.soukup@gmail.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling and Fine Tuning\n\n\u003e Owner: Daniel Soukup - Created: 2025.11.01\n\nIn this notebook, we load the processed data and fit our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s load our processed data and create feature/target dataframes for both train and test."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\n# Read recipe inputs\nprocessed_learn \u003d dataiku.Dataset(\"processed_learn\")\nprocessed_learn_df \u003d processed_learn.get_dataframe()\n\nprocessed_test \u003d dataiku.Dataset(\"processed_test\")\nprocessed_test_df \u003d processed_test.get_dataframe()\n\nprocessed_learn_df.shape, processed_test_df.shape"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "processed_learn_df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TARGET \u003d \u0027income\u0027"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train, y_train \u003d processed_learn_df.drop(columns\u003dTARGET), processed_learn_df[TARGET]\nX_test, y_test \u003d processed_test_df.drop(columns\u003dTARGET), processed_test_df[TARGET]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall that 8% of the processed samples fall into the target class 1 (high income) so a dummy classifier predicting 0 only would be 92% accurate."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_train.mean()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Important Note:** We won\u0027t use the test set for any optimization to avoid overfitting, we reserve the test set for final evaluation only of the optimized model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modeling\n\nOur current approach will focus on optimizin an XGBoost binary classifier. We do this using Optuna to search the hyperparameter space efficiently. We also aim to address the class imbalance during the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit Baseline"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from typing import Dict, Any\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\ndef cross_val_score_xgb(param: Dict[str, Any]) -\u003e float:\n    \"\"\"\n    Fit model with cross validation using the provided params.\n    \n    Return the avg out-of-fold accuracy.\n    \"\"\"\n    dtrain \u003d xgb.DMatrix(X_train, label\u003dy_train)\n\n    results \u003d xgb.cv(\n        params\u003dparam,\n        dtrain\u003ddtrain,\n        nfold\u003d3,\n        seed\u003d42,\n        verbose_eval\u003dFalse,\n        stratified\u003dparam.get(\"stratified_cv\", False),\n    )\n    \n    return results\n\n# we wont change these\nBASE_PARAMS \u003d {\n    \"verbosity\": 0,\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": \"auc\",\n    \"stratified_cv\": True\n}\n\nparam \u003d BASE_PARAMS.copy()\nparam.update({\n    \"n_estimators\": 50,\n    \"max_depth\": 2,\n    }\n)\n\ncross_val_score_xgb(param)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimize Hyperparameters\n\nNext, we\u0027ll look to optimize the model hyperparameters."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def objective(trial):\n    \"\"\"\n    Capture a single param combination and model fitting,\n    evaluated using cross-validation.\n    \"\"\"\n    param \u003d BASE_PARAMS.copy()\n    param.update({\n        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0), # default 1 - all rows\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0), # default 1 - all columns\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200, step\u003d10), # default 100\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 20, step\u003d2) # default 3\n    })\n    \n    result \u003d cross_val_score_xgb(param)\n    \n    trial.set_user_attr(\"n_estimators\", len(result))\n    \n    best_score \u003d result[\"test-auc-mean\"].values[-1]\n    \n    return best_score"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import optuna\n\nstudy \u003d optuna.create_study(direction\u003d\"maximize\")\n\nstudy.optimize(objective, n_trials\u003d3, timeout\u003d600)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets see the best results:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Best trial:\")\ntrial \u003d study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\n\nproject \u003d dataiku.api_client().get_default_project()\nmanaged_folder \u003d project.get_managed_folder(\u0027lV6oqreY\u0027)\n\nwith project.setup_mlflow(managed_folder\u003dmanaged_folder) as mlflow_handle:\n\n    # Note: if you don\u0027t call this (i.e. when no experiment is specified), the default one is used\n    mlflow_handle.set_experiment(\"test\")\n\n    with mlflow_handle.start_run(run_name\u003d\"my_run\"):\n        # ...your MLflow code...\n        mlflow_handle.log_param(\"a\", 1)\n        mlflow_handle.log_metric(\"b\", 2)\n\n        # This uses the regular MLflow APIs"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Refit Best Model\n\nNow that we found the best parameters, we refit on the whole dataset:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model \u003d XGBClassifier(**study.best_trial.params)\nmodel \u003d model.fit(X_train, y_train)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "model.score(X_train, y_train)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can get a quick sense of model accuracy however this number is quite unreliable (we achieved slightly better result than our dummy all-0 prediction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predict\n\nWe save the predicted class and probabilities both calculated:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "predictions_learn_df \u003d pd.DataFrame(\n    {\n        TARGET: y_train,\n        \u0027pred\u0027: model.predict(X_train),\n        \u0027pred_proba\u0027: model.predict_proba(X_train)[:, 1]\n    }\n)\n\npredictions_test_df \u003d pd.DataFrame(\n    {\n        TARGET: y_test,\n        \u0027pred\u0027: model.predict(X_test),\n        \u0027pred_proba\u0027: model.predict_proba(X_test)[:, 1]\n    }\n)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save predictions\n\nWe finally save the results to their own datasets which can be used for evaluation:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write recipe outputs\npredictions_learn \u003d dataiku.Dataset(\"predictions_learn\")\npredictions_learn.write_with_schema(predictions_learn_df)\n\npredictions_test \u003d dataiku.Dataset(\"predictions_test\")\npredictions_test.write_with_schema(predictions_test_df)"
      ],
      "outputs": []
    }
  ]
}