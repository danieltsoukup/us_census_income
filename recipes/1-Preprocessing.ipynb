{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-us_census_project-vcpu-4-ram-32gib",
      "display_name": "Python in vCPU-4-RAM-32GiB (env us_census_project)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "tags": [
      "recipe-editor"
    ],
    "createdOn": 1762023646408,
    "associatedRecipe": "compute_processed_learn",
    "creator": "daniel.t.soukup@gmail.com",
    "customFields": {},
    "modifiedBy": "daniel.t.soukup@gmail.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing\n\nWe will process our data set and prepare it for modelling based on our EDA findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data "
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\ndef load_data_by_name(name: str) -\u003e pd.DataFrame:\n    \"\"\"\n    Load dataset by its name.\n    \"\"\"\n    mydataset \u003d dataiku.Dataset(name)\n    mydataset_df \u003d mydataset.get_dataframe()\n    \n    return mydataset_df\n\ntrain_df \u003d load_data_by_name(\"census_income_learn\")\ntest_df \u003d load_data_by_name(\"census_income_test\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TARGET \u003d \"income\""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "col_mapping \u003d {\n    \"col_0\": \"age\", # matches type and range\n    \"col_1\": \"class of worker\", # unique values checked with data dict (UVDD)\n    \"col_2\": \"detailed industry recode\", # UVDD\n    \"col_3\": \"detailed occupation recode\", # UVDD\n    \"col_4\": \"education\", # UVDD\n    \"col_5\": \"wage per hour\", # looks to be at right position, type checks, in cents?\n    \"col_6\": \"enroll in edu inst last wk\", # UVDD\n    \"col_7\": \"marital stat\", # UVDD\n    \"col_8\": \"major industry code\", # UVDD\n    \"col_9\": \"major occupation code\", # UVDD\n    \"col_10\": \"race\", # UVDD\n    \"col_11\": \"hispanic origin\", # UVDD - 10 unique in data dict? values match though\n    \"col_12\": \"sex\", # UVDD\n    \"col_13\": \"member of a labor union\", # UVDD\n    \"col_14\": \"reason for unemployment\", # UVDD\n    \"col_15\": \"full or part time employment stat\", # UVDD\n    \"col_16\": \"capital gains\", # data dict check, range ok, dollars?\n    \"col_17\": \"capital losses\", # data dict check, range ok, dollars?\n    \"col_18\": \"dividends from stocks\", # data dict check\n    \"col_19\": \"tax filer stat\", # UVDD\n    \"col_20\": \"region of previous residence\", # UVDD\n    \"col_21\": \"state of previous residence\", # UVDD\n    \"col_22\": \"detailed household and family stat\", # data dict check\n    \"col_23\": \"detailed household summary in household\", # data dict check\n    \"col_24\": \"instance weight\", # SPECIAL\n    \"col_25\": \"migration code-change in msa\", # UVDD\n    \"col_26\": \"migration code-change in reg\", # UVDD\n    \"col_27\": \"migration code-move within reg\", # UVDD\n    \"col_28\": \"live in this house 1 year ago\",# UVDD\n    \"col_29\": \"migration prev res in sunbelt\", # UVDD\n    \"col_30\": \"num persons worked for employer\", # value check\n    \"col_31\": \"family members under 18\", # UVDD\n    \"col_32\": \"country of birth mother\",  # UVDD\n    \"col_33\": \"country of birth self\",  # UVDD\n    \"col_34\": \"country of birth father\",  # UVDD\n    \"col_35\": \"citizenship\", # UVDD\n    \"col_36\": \"own business or self employed\", # UVDD\n    \"col_37\": \"fill inc questionnaire for veteran\u0027s admin\", # UVDD\n    \"col_38\": \"veterans benefits\", # UVDD\n    \"col_39\": \"weeks worked in year\", # data dict order\n    \"col_40\": \"year\", # UVDD\n    \"col_41\": \"income\"\n}"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_to_object_cols \u003d [\n    \"detailed industry recode\", \n    \"detailed occupation recode\",\n    \"own business or self employed\",\n    \"veterans benefits\",\n    \"year\",\n    \"num persons worked for employer\",\n]\n\ncol_type_map \u003d {col: \"object\" for col in num_to_object_cols}"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Column mapping and dtypes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will organize the workflow into a single pipeline of transformers. This helps to ensure we apply the same transformations to train and test, avoid data leakage, and creates more easily maintainable code."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn import set_config\nset_config(transform_output\u003d\"pandas\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, OneToOneFeatureMixin\nfrom typing import Dict\n\nclass MapColumns(TransformerMixin, BaseEstimator, OneToOneFeatureMixin):\n    def __init__(self, *, col_name_map: Dict[str, str], col_type_map: Dict[str, str]):\n        self.col_name_map \u003d col_name_map\n        self.col_type_map \u003d col_type_map\n        self.feature_names_out_ \u003d None\n\n    def fit(self, X, y\u003dNone):\n        self.n_features_in_ \u003d X.shape[1]\n        return self\n    \n    def transform(self, X):\n        X_out \u003d X.copy()\n        \n        X_out \u003d X_out.rename(columns\u003dself.col_name_map)\n        \n        for col, type_ in self.col_type_map.items():\n            X_out[col] \u003d X_out[col].astype(type_)\n        \n        self.feature_names_out_ \u003d X_out.columns\n        \n        return X_out\n    \n    def get_feature_names_out(self, input_features\u003dNone):\n        return self.feature_names_out_\n\n    \nmapper_transformation \u003d MapColumns(\n    col_name_map\u003dcol_mapping,\n    col_type_map\u003dcol_type_map\n)\n\n# test\nsample \u003d mapper_transformation.fit_transform(train_df.head(100))\nsample"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mapper_transformation.get_feature_names_out()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning\n\nWe deal with missing values and duplicates as proposed in the EDA notebook."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from typing import List\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\nclass DropDuplicates(TransformerMixin, BaseEstimator, OneToOneFeatureMixin):\n    def __init__(self, *, ignore_cols: List[str]):\n        self.ignore_cols \u003d ignore_cols\n    \n    def fit(self, X, y\u003dNone):\n        self.n_features_in_ \u003d X.shape[1]\n        self.feature_names_out_ \u003d list(X.columns)\n        \n        return self\n    \n    def transform(self, X):\n        subset \u003d X.drop(columns\u003dself.ignore_cols).columns\n        \n        return X.drop_duplicates(subset\u003dsubset)\n    \n    def get_feature_names_out(self, input_features\u003dNone):\n        return self.feature_names_out_\n\n### Imputation ###\n\nimputer_transformation \u003d ColumnTransformer(\n    [\n        (\u0027impute missing\u0027, SimpleImputer(strategy\u003d\u0027constant\u0027, fill_value\u003d\"Do not know\"), [\u0027hispanic origin\u0027])\n    ],\n    remainder\u003d\u0027passthrough\u0027,\n    verbose_feature_names_out\u003dFalse,\n)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dropper_transformation \u003d DropDuplicates(ignore_cols\u003d[\u0027instance weight\u0027, \u0027income\u0027])\n\nsample \u003d dropper_transformation.fit_transform(sample)\nsample \u003d imputer_transformation.fit_transform(sample)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sample.shape"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sample.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract categorical features\n\nFirst, lets take those features from the EDA which showed the most singificant relationship with our target."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "columns_to_dummy \u003d [\n    \u0027class of worker\u0027,\n    \u0027sex\u0027,\n    \u0027education\u0027,\n    \u0027marital stat\u0027,\n    \u0027full or part time employment stat\u0027,\n]\n\nnum_columns \u003d [\n    \"age\", \"wage per hour\", \"capital gains\", \"capital losses\", \"dividends from stocks\", \"weeks worked in year\"\n]\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also take this step to encode the target:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sample[TARGET].unique()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TargetEncode(TransformerMixin, BaseEstimator, OneToOneFeatureMixin):\n    \"\"\"\n    Encode a binary target keeping a specified category.\n    \"\"\"\n    def __init__(self, col_keep: str):\n        self.col_keep \u003d col_keep\n    \n    def fit(self, X, y\u003dNone):\n        self.n_features_in_ \u003d X.shape[1]\n        self.feature_names_out_ \u003d list(X.columns)\n        \n        return self\n    \n    def transform(self, X):\n        X_out \u003d X.copy()\n        X_out[TARGET] \u003d np.where(X_out[TARGET] \u003d\u003d self.col_keep, 1, 0)\n        \n        return X_out\n    \n    def get_feature_names_out(self, input_features\u003dNone):\n        return self.feature_names_out_\n    \nclass PassthroughTransformer(TransformerMixin, BaseEstimator, OneToOneFeatureMixin):\n    \"\"\"\n    Helper to keep some columns.\n    \"\"\"\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y\u003dNone):\n        self.n_features_in_ \u003d X.shape[1]\n        self.feature_names_out_ \u003d list(X.columns)\n        \n        return self\n    \n    def transform(self, X):\n        return X\n    \n    def get_feature_names_out(self, input_features\u003dNone):\n        return self.feature_names_out_\n    "
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\ndummy_encoder \u003d ColumnTransformer(\n    [\n        (\u0027dummy_encode\u0027, OneHotEncoder(drop\u003d\"first\", min_frequency\u003d0.05, sparse_output\u003dFalse), columns_to_dummy),\n        (\u0027target_encode\u0027, TargetEncode(col_keep\u003d\"50000+.\"), [TARGET]),\n        (\u0027passthrough\u0027, PassthroughTransformer(), num_columns)\n    ],\n    verbose_feature_names_out\u003dFalse,\n    remainder\u003d\u0027drop\u0027,\n)\n\ndummy_encoder.fit_transform(sample)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define and Run the Preprocessing Pipeline "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we are ready to put together our pipeline:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "### DEFINE ###\n\n# column mapping\nmapper_transformation \u003d MapColumns(\n    col_name_map\u003dcol_mapping,\n    col_type_map\u003dcol_type_map\n)\n\n# duplicates\ndropper_transformation \u003d DropDuplicates(ignore_cols\u003d[\u0027instance weight\u0027, \u0027income\u0027])\n\n# nulls\nimputer_transformation \u003d ColumnTransformer(\n    [\n        (\u0027impute missing\u0027, SimpleImputer(strategy\u003d\u0027constant\u0027, fill_value\u003d\"Do not know\"), [\u0027hispanic origin\u0027])\n    ],\n    remainder\u003d\u0027passthrough\u0027,\n    verbose_feature_names_out\u003dFalse,\n)\n\n\n# final encoding - keeping numeric columns as is\ndummy_encoder \u003d ColumnTransformer(\n    [\n        (\u0027dummy_encode\u0027, OneHotEncoder(drop\u003d\"first\", min_frequency\u003d0.05, sparse_output\u003dFalse), columns_to_dummy),\n        (\u0027target_encode\u0027, TargetEncode(col_keep\u003d\"50000+.\"), [TARGET]),\n        (\u0027passthrough\u0027, PassthroughTransformer(), num_columns)\n    ],\n    verbose_feature_names_out\u003dFalse,\n    remainder\u003d\u0027drop\u0027,\n)\n\n\n### COMPOSE ###\n\npreprocessing_pipeline \u003d Pipeline(\n    [\n        (\u0027map_columns\u0027, mapper_transformation),\n        (\u0027drop_duplication\u0027, dropper_transformation),\n        (\u0027impute_null\u0027, imputer_transformation),\n        (\u0027dummy_encoder\u0027, dummy_encoder)\n    ]\n)\n\n### FIT ###\n\nfitted_pipeline \u003d preprocessing_pipeline.fit(train_df)\n\n### TRANSFORM ###\n\ntrain_processed \u003d fitted_pipeline.transform(train_df)\ntest_processed \u003d fitted_pipeline.transform(test_df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_processed.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_processed.shape, test_processed.shape"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sanity Checks\n\nThese need to pass before we can save the data."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def validate_processed(df: pd.DataFrame) -\u003e None:\n    \"\"\"\n    Ensure key attributes before saving data.\n    \"\"\"\n    # no nulls\n    assert df.isna().sum().sum() \u003d\u003d 0\n    \n    # target present\n    assert TARGET in df.columns\n    \n    # all numeric\n    assert df.shape[1] \u003d\u003d df.select_dtypes(\"number\").shape[1]\n    \nvalidate_processed(train_processed)\nvalidate_processed(test_processed)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Data "
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write recipe outputs\nprocessed_learn \u003d dataiku.Dataset(\"processed_learn\")\nprocessed_learn.write_with_schema(train_processed)\n\nprocessed_test \u003d dataiku.Dataset(\"processed_test\")\nprocessed_test.write_with_schema(test_processed)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}