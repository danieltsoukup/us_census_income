{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-us_census_project-vcpu-2-ram-16gib",
      "display_name": "Python in vCPU-2-RAM-16GiB (env us_census_project)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "tags": [
      "recipe-editor"
    ],
    "createdOn": 1762041605966,
    "associatedRecipe": "3-Evaluation-recipe",
    "creator": "daniel.t.soukup@gmail.com",
    "customFields": {},
    "modifiedBy": "daniel.t.soukup@gmail.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation\n\nIn this final recipe, we run in-depth evaluation on our best performing model using a number of classification metrics and visuals. Again, our focus is to understand model accuracy in the general sense as well as specific shortcomings to motivate future model (and data processing) iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Prediction Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the prediction datasets for analysis."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\n# Read recipe inputs\npredictions_learn \u003d dataiku.Dataset(\"predictions_learn\")\npredictions_learn_df \u003d predictions_learn.get_dataframe()\n\npredictions_test \u003d dataiku.Dataset(\"predictions_test\")\npredictions_test_df \u003d predictions_test.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "predictions_learn_df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TARGET \u003d \u0027income\u0027"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "datasets \u003d {\n    \u0027train\u0027: predictions_learn_df,\n    \u0027test\u0027: predictions_test_df\n}"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction Statistics "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets look at high level statistics of the predictions first."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for name, data in datasets.items():\n    print(name, \u0027summary:\u0027)\n    print(data.describe())\n    \n    print(\u0027\\n\\n\u0027)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can already see that while the real labels contain ~8% high income, our predictions were positive only ~4% of the time. However, the predicted probabilities are fairly well calibrated showing the same 0.08 mean. We can definitely experiement with alternative cutoffs to the default 0.5 to find better precision/recall trade-offs."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.express as px\n\nimport plotly.offline as pyo\npyo.init_notebook_mode()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig \u003d datasets[\u0027train\u0027].plot(kind\u003d\u0027hist\u0027, backend\u003d\u0027plotly\u0027, x\u003d\u0027pred_proba\u0027, color\u003d\u0027income\u0027, log_y\u003dTrue, opacity\u003d0.7)\nfig.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see the probability distribution for the true 0/1 labels. For class 0, we are nicely concentrated on 0 as expected, however we do see a fair number of class 1 samples with low probabilities (e.g., instances misclassified by a large margin).\n\nAs future work, we can explore the segment of data where these misclassifications occurred to better understand how to address the issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Metrics \n\nWe will look at the standard binary classification metrics using:\n- confusion matrices\n- classification reports (accuracy, precision, recall, F1 score)\n- ROC-AUC curves"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n\ndef get_classification_reports(datasets) -\u003e None:\n    for name, data in datasets.items():\n        print(name, \"\\n\\n\")\n        \n        print(classification_report(data[TARGET], data[\u0027pred\u0027]))\n        \n        print(\"\\n\\n\")\n        \nget_classification_reports(datasets)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observations:\n    - The train and test precision/recall scores are quite close which is a positive sign that there is no overfitting.\n    - The F1 score (for our class 1) are around 0.5 on both sets with .75-.77 precision (% of predicted 1 being actually correct) and .37 recall (% of real label 1 samples being predicted 1). This is quite common for imbalanced datasets and the minority class.\n    - Overall accuracy is at 94% however this is not a meaningful metric on such imbalanced data with only 8% of samples in the minority class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confusion matrices give another view of the correct and misclassified samples:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "import matplotlib.pyplot as plt\n\nConfusionMatrixDisplay.from_predictions(predictions_learn_df[\u0027income\u0027], predictions_learn_df[\u0027pred\u0027])\nplt.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ConfusionMatrixDisplay.from_predictions(predictions_test_df[\u0027income\u0027], predictions_test_df[\u0027pred\u0027])\nplt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Precision-Recall Curves\n\nAs mentioned previously, we can consider adjusting our hard predictions to achieve a better precision-recall tradeoff. We can see this on the precision-recall curve - note that we used the area under this curve as the loss function to optimize during training our XGBoost models."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import PrecisionRecallDisplay\n\ndisplay \u003d PrecisionRecallDisplay.from_predictions(\n    datasets[\u0027test\u0027][TARGET],\n    datasets[\u0027test\u0027][\u0027pred_proba\u0027],\n    name\u003d\"XGBoost\",\n    plot_chance_level\u003dTrue,\n    despine\u003dTrue\n)\nplt.axvline(0.37, c\u003d\"black\") # mark the recall-precion we got from eval\nplt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We could select an alternive threshold:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import precision_recall_curve\n\nprecision, recall, thresholds \u003d precision_recall_curve(\n    datasets[\u0027test\u0027][TARGET],\n    datasets[\u0027test\u0027][\u0027pred_proba\u0027],\n)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.DataFrame(\n    {\n        \u0027precision\u0027: precision[::1500],\n        \u0027recall\u0027: recall[::1500],\n        \u0027threshold\u0027: thresholds[::1500]\n    }\n).set_index(\u0027threshold\u0027).T"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that if we are to sacrifice some precision, the recall can be brought up .e.g, with 0.255 threshold we can achieve an almost even 0.55 precision and 0.57 recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ROC-AUC\n\nLets look at the ROC-AUC scores and ROC curve finally:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_roc_scores(datasets) -\u003e None:\n    for name, data in datasets.items():\n        print(name)\n        print(roc_auc_score(data[TARGET], data[\u0027pred_proba\u0027]))\n        \n        print(\"\\n\")\n        \nget_roc_scores(datasets)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RocCurveDisplay.from_predictions(predictions_test_df[TARGET], predictions_test_df[\"pred_proba\"])\nRocCurveDisplay.from_predictions(predictions_learn_df[TARGET], predictions_learn_df[\"pred_proba\"])\nplt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ideally, the curve is hugging the top left corner (as we see it) but again, this metric is not sensitive enough for imbalanced datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n\nTo better understand model performance:\n- we can look at the misclassified samples and explore the features and how they differ from the general population.\n- we should more carefully consider what metric to use as training loss and monitoring during HP optimization."
      ]
    }
  ]
}