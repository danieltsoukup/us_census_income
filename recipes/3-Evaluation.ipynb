{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-us_census_project-vcpu-4-ram-32gib",
      "display_name": "Python in vCPU-4-RAM-32GiB (env us_census_project)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "tags": [
      "recipe-editor"
    ],
    "createdOn": 1762041605966,
    "associatedRecipe": "compute_3NRRYyl1",
    "creator": "daniel.t.soukup@gmail.com",
    "customFields": {},
    "modifiedBy": "daniel.t.soukup@gmail.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation\n\nIn this final recipe, we run evaluation on our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Prediction Data "
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\n# Read recipe inputs\npredictions_learn \u003d dataiku.Dataset(\"predictions_learn\")\npredictions_learn_df \u003d predictions_learn.get_dataframe()\n\npredictions_test \u003d dataiku.Dataset(\"predictions_test\")\npredictions_test_df \u003d predictions_test.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "predictions_learn_df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TARGET \u003d \u0027income\u0027"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "datasets \u003d {\n    \u0027train\u0027: predictions_learn_df,\n    \u0027test\u0027: predictions_test_df\n}"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Metrics \n\nWe will look at the standard binary classification metrics using:\n- confusion matrices\n- classification reports (accuracy, precision, recall, F1 score)\n- ROC-AUC curves"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "classification_report()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n\ndef get_classification_reports(datasets) -\u003e None:\n    for name, data in datasets.items():\n        print(name, \"\\n\\n\")\n        \n        print(classification_report(data[TARGET], data[\u0027pred\u0027]))\n        \n        print(\"\\n\\n\")\n        \nget_classification_reports(datasets)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confusion matrices:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "ConfusionMatrixDisplay.from_predictions(predictions_learn_df[\u0027income\u0027], predictions_learn_df[\u0027pred\u0027])\nplt.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ConfusionMatrixDisplay.from_predictions(predictions_test_df[\u0027income\u0027], predictions_test_df[\u0027pred\u0027])\nplt.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ROC-AUC scores finally:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_roc_scores(datasets) -\u003e None:\n    for name, data in datasets.items():\n        print(name)\n        print(roc_auc_score(data[TARGET], data[\u0027pred\u0027]))\n        \n        print(\"\\n\")\n        \nget_roc_scores(datasets)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RocCurveDisplay.from_predictions(predictions_test_df[TARGET], 1-predictions_test_df[\"pred_proba\"])\nRocCurveDisplay.from_predictions(predictions_train_df[TARGET], 1-predictions_train_df[\"pred_proba\"])\nplt.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Interpretation "
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Eval "
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write recipe outputs\nxgboost_eval \u003d dataiku.ModelEvaluationStore(\"3NRRYyl1\")\nxgboost_eval_info \u003d xgboost_eval.get_info()"
      ],
      "outputs": []
    }
  ]
}